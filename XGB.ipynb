{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for building an XGBoost model for the Sendgrid sign up dataset\n",
    "***\n",
    "**Jake Mitchell Scott Schubert**\n",
    "\n",
    "Initially using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To do anything\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#Xgboost\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#random forrest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "#One-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_boi(col, expect_type):\n",
    "    values = df[col]\n",
    "    # integer encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    values = label_encoder.fit_transform(values.astype(expect_type))\n",
    "    integer_encoded = label_encoder.fit_transform(values)\n",
    "    # binary encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    return onehot_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  banned_ip  ip_count  is_authy_verified  banned_email  is_transactional  \\\n",
      "0   0          6         8                  0             1                 0   \n",
      "1   1          5         5                  0             1                 1   \n",
      "2   2          0         1                  0             0                 1   \n",
      "3   3          4         4                  0             2                 0   \n",
      "4   4          0         1                  0             0                 1   \n",
      "\n",
      "   is_marketing  is_behavioral  is_oem  geolocation_risk        ...          \\\n",
      "0             0              0       0                 0        ...           \n",
      "1             0              0       0                 0        ...           \n",
      "2             0              0       0               -16        ...           \n",
      "3             1              0       0               -14        ...           \n",
      "4             0              0       0               -16        ...           \n",
      "\n",
      "   marketing  technical_support  name_notes_inv  name_notes_firstlast  \\\n",
      "0          0                  0               0                     0   \n",
      "1          0                  0               0                     0   \n",
      "2          0                  0               0                     0   \n",
      "3          0                  0               0                     0   \n",
      "4          0                  0               0                     0   \n",
      "\n",
      "   name_notes_inv_firstlast  name_notes_sus_firstlast  name_notes_sus  \\\n",
      "0                         0                         0               0   \n",
      "1                         0                         0               0   \n",
      "2                         0                         0               0   \n",
      "3                         0                         0               0   \n",
      "4                         0                         0               0   \n",
      "\n",
      "   name_notes_inv_sus_firstlast  name_notes_inv_sus  name_notes_tagbad  \n",
      "0                             0                   0                  0  \n",
      "1                             0                   0                  0  \n",
      "2                             0                   0                  0  \n",
      "3                             0                   0                  0  \n",
      "4                             0                   0                  0  \n",
      "\n",
      "[5 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "train_path = 'data/signup_train_data.csv'\n",
    "test_path = 'data/signup_test_data.csv'\n",
    "\n",
    "\n",
    "# Load the data into a DataFrame \n",
    "df = pd.read_csv(train_path, low_memory=False)\n",
    "test_df = pd.read_csv(test_path, low_memory=False)\n",
    "\n",
    "# Split into X and Y\n",
    "dfLabels = df.pop(\"label\")\n",
    "\n",
    "\n",
    "#Feature engineering\n",
    "'''\n",
    "    Giving up on one-hot-encoding for now.  Not converting to \n",
    "    a friendly data format for our pandas dataFrame and I'm not \n",
    "    convinced it will assist very much and would rather invest time\n",
    "    into feature engineering specific columns\n",
    "state_ohe = one_hot_boi('state', 'str')\n",
    "aoeu = pd.DataFrame(state_ohe.T)\n",
    "df['state_ohe'] = 0\n",
    "count = 0\n",
    "for row in state_ohe:\n",
    "    aoeu = pd.DataFrame(row)\n",
    "    df['state_ohe'].append(aoeu)\n",
    "    print(aoeu)\n",
    "'''\n",
    "\n",
    "\n",
    "#Feature Engineering\n",
    "\n",
    "for dataframe in [df, test_df]:\n",
    "    \n",
    "    dataframe['employee_count_high'] = 0\n",
    "    dataframe.loc[df['employee_count']  == '1 - 500', 'employee_count_high'] = 500\n",
    "    dataframe.loc[df['employee_count']  == '501 - 1,000', 'employee_count_high'] = 1000\n",
    "    dataframe.loc[df['employee_count']  == '1,001 - 5,000', 'employee_count_high'] = 5000\n",
    "    dataframe.loc[df['employee_count']  == '5,000+', 'employee_count_high'] = 10000\n",
    "    dataframe['volume_high'] = 0\n",
    "    dataframe.loc[df['volume']  == '1 - 100,000', 'volume_high'] = 100000\n",
    "    dataframe.loc[dataframe['volume']  == '100,001 - 2,500,000', 'volume_high'] = 2500000\n",
    "    dataframe.loc[dataframe['volume']  == '2,500,001 - 10,000,000', 'volume_high'] = 10000000\n",
    "    dataframe.loc[dataframe['volume']  == '10,000,000+', 'volume_high'] = 20000000\n",
    "    dataframe.loc[dataframe['volume']  == '0 to 40k', 'volume_high'] = 40000\n",
    "    dataframe.loc[dataframe['volume']  == '40k to 100k', 'volume_high'] = 100000\n",
    "    dataframe['developer'] = 0\n",
    "    dataframe.loc[dataframe['user_persona']  == 'Developer', 'developer'] = 1\n",
    "    dataframe['ceo'] = 0\n",
    "    dataframe.loc[dataframe['user_persona']  == 'CEO', 'ceo'] = 1\n",
    "    dataframe['other'] = 0\n",
    "    dataframe.loc[dataframe['user_persona']  == 'Other', 'other'] = 1\n",
    "    dataframe['marketing'] = 0\n",
    "    dataframe.loc[dataframe['user_persona']  == 'Marketing', 'marketing'] = 1\n",
    "    dataframe['technical_support'] = 0\n",
    "    dataframe.loc[dataframe['user_persona']  == 'Technical Support', 'technical_support'] = 1\n",
    "    \n",
    "    dataframe['name_notes_inv'] = 0\n",
    "    dataframe.loc[dataframe['name_notes']  == 'Invalid', 'name_notes_inv'] = 1\n",
    "    dataframe['name_notes_firstlast'] = 0\n",
    "    dataframe.loc[dataframe['name_notes']  == 'First equals Last', 'name_notes_firstlast'] = 1\n",
    "    dataframe['name_notes_inv_firstlast'] = 0\n",
    "    dataframe.loc[dataframe['name_notes']  == 'Invalid|First equals Last', 'name_notes_inv_firstlast'] = 1\n",
    "    dataframe['name_notes_sus_firstlast'] = 0\n",
    "    dataframe.loc[dataframe['name_notes']  == 'Suspect|First equals Last', 'name_notes_sus_firstlast'] = 1\n",
    "    dataframe['name_notes_sus'] = 0\n",
    "    dataframe.loc[dataframe['name_notes']  == 'Suspect', 'name_notes_sus'] = 1\n",
    "    dataframe['name_notes_inv_sus_firstlast'] = 0\n",
    "    dataframe.loc[dataframe['name_notes']  == 'Invalid|Suspect|First equals Last', 'name_notes_inv_sus_firstlast'] = 1\n",
    "    dataframe['name_notes_inv_sus'] = 0\n",
    "    dataframe.loc[dataframe['name_notes']  == 'Invalid|Suspect', 'name_notes_inv_sus'] = 1\n",
    "    dataframe['name_notes_tagbad'] = 0\n",
    "    dataframe.loc[dataframe['name_notes']  == 'Tagged Bad', 'name_notes_tagbad'] = 1\n",
    "\n",
    "\n",
    "#alternate attepmt\n",
    "#UPDATE:  Works as intended however cannot be read in during learning algo\n",
    "# They need \"int, float, or bool\" therefore this is a bus\n",
    "'''name_note_ohe = one_hot_boi('name_notes', 'str')\n",
    "nn_df = pd.DataFrame(index=range(0,len(df)),columns=['name_note_ohe'])\n",
    "for i in range(0,len(df)):\n",
    "    nn_df['name_note_ohe'][i] = name_note_ohe[i]\n",
    "    if i % 1000 == 1:\n",
    "        print(i)\n",
    "    \n",
    "df['name_note_ohe'] = nn_df['name_note_ohe']\n",
    "'''\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#Temporarily parse non numerical data\n",
    "frames_to_remove = ['risk','zip', 'city', 'website', 'state', 'country', 'registration_ip', 'company','multifactor_country_code','created_at','lead_source','marketing_channel','volume','user_persona','initial_package','employee_count','geolocation_notes','name_notes','ip_notes','community_notes','email_notes','activity_notes','fingerprint_notes']\n",
    "df = df.drop(frames_to_remove, axis = 1)\n",
    "test_df = test_df.drop(frames_to_remove, axis = 1)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(X):\n",
    "    '''Takes a row of X inputs and returns the predicted label'''\n",
    "    if X[\"banned_ip\"]: #this alone is ~92%\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def bannedIP(seed_in, TO_TEST):\n",
    "    # split data into train and test sets\n",
    "    seed = seed_in\n",
    "    test_size = 0.5\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, dfLabels, test_size=test_size, random_state=seed)\n",
    "\n",
    "    # fit model no training data\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = [get_label(x) for i,x in X_test.iterrows()]\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"xgb - Seed: %i    -  Accuracy: %.2f%%\" % (seed,accuracy * 100.0))\n",
    "\n",
    "    if TO_TEST:\n",
    "        #make predictions for train data\n",
    "        y_pred = [get_label(x) for i,x in test_df.iterrows()]\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "\n",
    "        d = {'id':test_df['id'], 'label':predictions}\n",
    "        preds = pd.DataFrame(data=d)\n",
    "\n",
    "        preds.to_csv('predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost approach \n",
    "\n",
    "def xgb(seed_in, TO_TEST):\n",
    "    # split data into train and test sets\n",
    "    seed = seed_in\n",
    "    test_size = 0.7\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, dfLabels, test_size=test_size, random_state=seed)\n",
    "\n",
    "    # fit model no training data\n",
    "    model = XGBClassifier(depth = 4, eta = 0.01, n_estimators = 120)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"xgb - Seed: %i    -  Accuracy: %.2f%%\" % (seed,accuracy * 100.0))\n",
    "    \n",
    "    if TO_TEST:\n",
    "        #make predictions for train data\n",
    "        y_pred = model.predict(test_df)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "\n",
    "        d = {'id':test_df['id'], 'label':predictions}\n",
    "        preds = pd.DataFrame(data=d)\n",
    "\n",
    "        preds.to_csv('predictions.csv', index=False)\n",
    "        return model.predict_proba(test_df)\n",
    "        \n",
    "    return model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xgb(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forrest approach \n",
    "\n",
    "def rf(seed, TO_TEST):\n",
    "    seed = seed\n",
    "    df2 = df\n",
    "    df2 = df2.fillna(0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df2, dfLabels, test_size=0.7, random_state=seed)\n",
    "\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
    "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                min_samples_leaf=1, min_samples_split=2,\n",
    "                min_weight_fraction_leaf=0.0, n_estimators=27, n_jobs=1,\n",
    "                oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "\n",
    "    # Use the forest's predict method on the test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"RF - Seed: %i    -  Accuracy: %.2f%%\" % (seed,accuracy * 100.0))\n",
    "    \n",
    "    if TO_TEST:\n",
    "        test_df2 = test_df\n",
    "        test_df2 = test_df2.fillna(0)\n",
    "        \n",
    "        #make predictions for train data\n",
    "        y_pred2 = clf.predict(test_df2)\n",
    "        predictions = [round(value) for value in y_pred2]\n",
    "\n",
    "        d = {'id':test_df['id'], 'label':predictions}\n",
    "        preds = pd.DataFrame(data=d)\n",
    "\n",
    "        preds.to_csv('predictions.csv', index=False)\n",
    "        return clf.predict_proba(test_df2)\n",
    "    \n",
    "    return (clf.predict_proba(X_test), y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rf(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the prediction values of random forrest and xgboost \n",
    "\n",
    "\n",
    "def combine(seed, TO_TEST, rf_weight, xgb_weight):           \n",
    "    if TO_TEST:\n",
    "        x = rf(seed, TO_TEST)\n",
    "    else:\n",
    "        #Acquire training sample\n",
    "        (x,y_test) = rf(seed, TO_TEST)\n",
    "    rand_score = [b for a,b in x]\n",
    "    \n",
    "    xgb_score = xgb(seed, TO_TEST)\n",
    "    xgb_score = [b for a,b in xgb_score]   \n",
    "\n",
    "    predictions = []\n",
    "    for i in range(0,len(rand_score)):\n",
    "        val = rand_score[i]*rf_weight + xgb_score[i]*xgb_weight\n",
    "        predictions.append(val)\n",
    "        \n",
    "    predictions = [int(round(value)) for value in predictions]\n",
    "    \n",
    "    if TO_TEST:\n",
    "        d = {'id':test_df['id'], 'label':predictions}\n",
    "        preds = pd.DataFrame(data=d)\n",
    "        preds.to_csv('predictions.csv', index=False)\n",
    "        \n",
    "    else:\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        print(\"Combine - Seed: %i    -  Accuracy: %.2f%%\" % (seed,accuracy * 100.0))       \n",
    "    \n",
    "    \n",
    "#seed, TO_TEST, rand forest weighting, xgboost weighting\n",
    "#combine(1, True, 0.3, 0.7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPDATE:  This was not helpful at all.  My approach of using two features (random forrest and xgb) led \n",
    "# to two different individual accuracies from these algorithms.  The combined 'prediciton' of the two was a linear\n",
    "#relationship between the lower score and its algorithm's weighting and likewise for the higher scored algorithm\n",
    "# ergo, this function is useless\n",
    "\n",
    "def combine_findWeight(seed):           \n",
    "    #Acquire training sample\n",
    "    (x,y_test) = rf(seed, False)\n",
    "    rand_score = [b for a,b in x]\n",
    "    \n",
    "    xgb_score = xgb(seed, False)\n",
    "    xgb_score = [b for a,b in xgb_score]   \n",
    "\n",
    "    print(\"SEED ------------ %i\" % (seed))\n",
    "    for rf_weight in range(1,100):\n",
    "        rf_weight = float(rf_weight/100)\n",
    "        xgb_weight = 1-rf_weight\n",
    "        print(rf_weight)\n",
    "        print(xgb_weight)\n",
    "        predictions = []\n",
    "        for i in range(0,len(rand_score)):\n",
    "            val = rand_score[i]*rf_weight + xgb_score[i]*xgb_weight\n",
    "            predictions.append(val)\n",
    "        predictions = [int(round(value)) for value in predictions]\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        print(\"rf . xgb = %f . %f  -  Accuracy: %.2f%%\" % (rf_weight, xgb_weight, accuracy * 100.0))  \n",
    "        print()\n",
    "\n",
    "    \n",
    "#seed, TO_TEST, rand forest weighting, xgboost weighting\n",
    "combine_findWeight(1)\n",
    "#for i in range(0,20):\n",
    "    #combine_findWeight(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost approach \n",
    "\n",
    "def xgbParam(seed_in, TO_TEST,depth=4,eta=0.01,n=120):\n",
    "    # split data into train and test sets\n",
    "    seed = seed_in\n",
    "    test_size = 0.7\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, dfLabels, test_size=test_size, random_state=seed)\n",
    "\n",
    "    # fit model no training data\n",
    "    model = XGBClassifier(depth = depth, eta = eta, n_estimators = n)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"xgb - Seed: %i    -  Accuracy: %.2f%%\" % (seed,accuracy * 100.0))\n",
    "    \n",
    "    if TO_TEST:\n",
    "        #make predictions for train data\n",
    "        y_pred = model.predict(test_df)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "\n",
    "        d = {'id':test_df['id'], 'label':predictions}\n",
    "        preds = pd.DataFrame(data=d)\n",
    "\n",
    "        preds.to_csv('predictions.csv', index=False)\n",
    "        return model.predict_proba(test_df)\n",
    "        \n",
    "    return model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/schubydooo/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb - Seed: 1    -  Accuracy: 97.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.6226044e-06, 9.9999738e-01],\n",
       "       [5.8851838e-03, 9.9411482e-01],\n",
       "       [4.3940824e-01, 5.6059176e-01],\n",
       "       ...,\n",
       "       [7.4755549e-03, 9.9252445e-01],\n",
       "       [4.0131211e-03, 9.9598688e-01],\n",
       "       [5.3048134e-05, 9.9994695e-01]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbParam(1,False,depth=7,eta=0.001,n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "96.99 - all\n",
    "96.28 - remove banned_ip\n",
    "97.00 - remove risk\n",
    "- remove both\n",
    "\n",
    "\n",
    "1- 2000\n",
    "2- 1400\n",
    "3- 900\n",
    "4- 450"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
